{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fuchuanzhe/MachineVision/blob/main/Machine_Vision_Final_Lab_Model_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boto3 -q\n",
        "!pip install opencv-python torch numpy torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4XrZuAWeoUc",
        "outputId": "704d370d-b09a-4427-c92f-1d02ca74b0d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/140.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/14.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/14.5 MB\u001b[0m \u001b[31m290.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m14.5/14.5 MB\u001b[0m \u001b[31m303.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.5/14.5 MB\u001b[0m \u001b[31m151.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/86.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the data"
      ],
      "metadata": {
        "id": "PMr99Yo7x8N1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data for this assignment has been made available and is downloadable to disk by running the below cell."
      ],
      "metadata": {
        "id": "4YhoE1nF2Pee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "from botocore import UNSIGNED\n",
        "from botocore.config import Config\n",
        "import os\n",
        "\n",
        "# Connect to S3 without authentication (public bucket)\n",
        "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
        "\n",
        "bucket_name = 'prism-mvta'\n",
        "prefix = 'training-and-validation-data/'\n",
        "download_dir = './video-data'\n",
        "\n",
        "os.makedirs(download_dir, exist_ok=True)\n",
        "\n",
        "# List all objects in the S3 path\n",
        "paginator = s3.get_paginator('list_objects_v2')\n",
        "pages = paginator.paginate(Bucket=bucket_name, Prefix=prefix)\n",
        "\n",
        "video_names = []\n",
        "\n",
        "for page in pages:\n",
        "    if 'Contents' not in page:\n",
        "        print(\"No files found at the specified path! Go and complain to the TAs!\")\n",
        "        break\n",
        "\n",
        "    for obj in page['Contents']:\n",
        "        key = obj['Key']\n",
        "        filename = os.path.basename(key)\n",
        "\n",
        "        if not filename:\n",
        "            continue\n",
        "\n",
        "        video_names.append(filename)\n",
        "\n",
        "        local_path = os.path.join(download_dir, filename)\n",
        "        print(f\"Downloading: {filename}\")\n",
        "        s3.download_file(bucket_name, key, local_path)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Downloaded videos:\")\n",
        "print(\"=\"*50)\n",
        "for name in video_names:\n",
        "    print(name)\n",
        "\n",
        "print(f\"\\nTotal: {len(video_names)} files\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5ekP01hR9VV",
        "outputId": "b7db6f1a-0f78-422e-9410-527901fdc226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 1_dksksjfwijf.mp4\n",
            "Downloading: 2_dfsaeklnvvalkej.mp4\n",
            "Downloading: 2_difficult_2.mp4\n",
            "Downloading: 2_difficult_sdafkljsalkfj.mp4\n",
            "Downloading: 2_dkdjwkndkfw.mp4\n",
            "Downloading: 2_dkdmkejkeimdh.mp4\n",
            "Downloading: 2_dkjd823kjf.mp4\n",
            "Downloading: 2_dsalkfjalwkenlke.mp4\n",
            "Downloading: 2_kling_20251205_Text_to_Video_On_a_sandy_4976_0.mp4\n",
            "Downloading: 2_kling_20251206_Text_to_Video_Generate_a_71_1.mp4\n",
            "Downloading: 2_sadfasjldkfjaseifj.mp4\n",
            "Downloading: 2_sdafkjaslkclaksdjkas.mp4\n",
            "Downloading: 2_sdfkjsaleijflaskdjf.mp4\n",
            "Downloading: 2_sdjfhafsldkjhjk.mp4\n",
            "Downloading: 2_sdkjdsflkjfwa.mp4\n",
            "Downloading: 2_sdlfjlewlkjkj.mp4\n",
            "Downloading: 2_sdlkjsaelijfksdjf.mp4\n",
            "Downloading: 3_asldkfjalwieaskdfaskdf.mp4\n",
            "Downloading: 3_dkk873lkjlksajdf.mp4\n",
            "Downloading: 3_dsjlaeijlksjdfie.mp4\n",
            "Downloading: 3_dsksdfjbvsdkj.mp4\n",
            "Downloading: 3_dslkaldskjflakjs.mp4\n",
            "Downloading: 3_ewdfkjwaeoihjlkasdjf.mp4\n",
            "Downloading: 3_kling_20251205_Text_to_Video_In_a_grass_4697_0.mp4\n",
            "Downloading: 3_kling_20251205_Text_to_Video_On_a_playg_5028_0.mp4\n",
            "Downloading: 3_kling_20251205_Text_to_Video_On_a_playg_5064_0.mp4\n",
            "Downloading: 3_kling_20251206_Text_to_Video_Generate_a_17_0.mp4\n",
            "Downloading: 3_kling_20251206_Text_to_Video_Generate_a_315_0.mp4\n",
            "Downloading: 3_kling_20251206_Text_to_Video_Generate_a_315_2.mp4\n",
            "Downloading: 3_kling_20251206_Text_to_Video_Generate_a_712_3.mp4\n",
            "Downloading: 3_kling_20251206_Text_to_Video_Generate_a_71_0.mp4\n",
            "Downloading: 3_kling_20251206_Text_to_Video_Generate_a_71_2.mp4\n",
            "Downloading: 3_kling_20251206_Text_to_Video_Generate_a_71_3.mp4\n",
            "Downloading: 3_kling_20251209_Image_to_Video_Generate_a_613_1.mp4\n",
            "Downloading: 3_kling_20251209_Image_to_Video_Generate_a_635_0.mp4\n",
            "Downloading: 3_kling_20251209_Text_to_Video_Generate_a_190_1.mp4\n",
            "Downloading: 3_kling_20251209_Text_to_Video_Generate_a_403_1.mp4\n",
            "Downloading: 3_kling_20251209_Text_to_Video_Generate_a_491_0.mp4\n",
            "Downloading: 3_kling_20251209_Text_to_Video_Generate_a_491_1.mp4\n",
            "Downloading: 3_kling_20251209_Text_to_Video_Generate_a_491_2.mp4\n",
            "Downloading: 3_kling_dskfseu.mp4\n",
            "Downloading: 3_kling_kdjflaskdjf.mp4\n",
            "Downloading: 3_sadklfjasbnlkjlfkj.mp4\n",
            "Downloading: 3_sadlfkjasldkfjasleijlkjfd.mp4\n",
            "Downloading: 3_sadlfkjawelnflksdjf.mp4\n",
            "Downloading: 3_sdfjwaiejflkasjdf.mp4\n",
            "Downloading: 3_sdflkjliejkjdf.mp4\n",
            "Downloading: 3_sdlkfjaleknaksej.mp4\n",
            "Downloading: 3_sdlkfjalkjejafe.mp4\n",
            "Downloading: 3_sdlkjfaslkjfalskjdf.mp4\n",
            "Downloading: 3_sdlkjslndflkseijlkjef.mp4\n",
            "Downloading: 4_20251209_Text_to_Video_Generate_a_561_0.mp4\n",
            "Downloading: 4_asdlkfjalsflnekj.mp4\n",
            "Downloading: 4_aslkcasckmwlejk.mp4\n",
            "Downloading: 4_aslkjasmcalkewjlkje.mp4\n",
            "Downloading: 4_dssalsdkfjweijf.mp4\n",
            "Downloading: 4_kling_20251206_Text_to_Video_Generate_a_28_0.mp4\n",
            "Downloading: 4_kling_20251206_Text_to_Video_Generate_a_315_3.mp4\n",
            "Downloading: 4_kling_20251206_Text_to_Video_Generate_a_58_0.mp4\n",
            "Downloading: 4_kling_20251207_Text_to_Video_Generate_a_521_1.mp4\n",
            "Downloading: 4_kling_20251209_Image_to_Video_Generate_a_635_1.mp4\n",
            "Downloading: 4_kling_20251209_Text_to_Video_Generate_a_190_0.mp4\n",
            "Downloading: 4_kling_20251209_Text_to_Video_Generate_a_218_0.mp4\n",
            "Downloading: 4_kling_20251209_Text_to_Video_Generate_a_263_1.mp4\n",
            "Downloading: 4_kling_20251209_Text_to_Video_Generate_a_377_1.mp4\n",
            "Downloading: 4_kling_20251209_Text_to_Video_Generate_a_452_0.mp4\n",
            "Downloading: 4_kling_20251209_Text_to_Video_Generate_a_452_1.mp4\n",
            "Downloading: 4_kling_20251209_Text_to_Video_Generate_a_561_1.mp4\n",
            "Downloading: 4_kling_20251209_Text_to_Video_Generate_a_588_2.mp4\n",
            "Downloading: 4_pushup_1f2da596-7619-4d55-9376-069e15a42a1a_h264.mp4\n",
            "Downloading: 4_sadflkjasldkjfalseij.mp4\n",
            "Downloading: 4_sadlfkjlknewkjejk.mp4\n",
            "Downloading: 5_sadfjhaslfkjasdlkfjsa.mp4\n",
            "Downloading: 5_sdfkljweoijlkjdsflkjweaij.mp4\n",
            "Downloading: 6_dfjewaijsldkjfsaef.mp4\n",
            "Downloading: 6_kling_20251209_Text_to_Video_Generate_a_218_1.mp4\n",
            "Downloading: 7_sadkjfkljekj.mp4\n",
            "\n",
            "==================================================\n",
            "Downloaded videos:\n",
            "==================================================\n",
            "1_dksksjfwijf.mp4\n",
            "2_dfsaeklnvvalkej.mp4\n",
            "2_difficult_2.mp4\n",
            "2_difficult_sdafkljsalkfj.mp4\n",
            "2_dkdjwkndkfw.mp4\n",
            "2_dkdmkejkeimdh.mp4\n",
            "2_dkjd823kjf.mp4\n",
            "2_dsalkfjalwkenlke.mp4\n",
            "2_kling_20251205_Text_to_Video_On_a_sandy_4976_0.mp4\n",
            "2_kling_20251206_Text_to_Video_Generate_a_71_1.mp4\n",
            "2_sadfasjldkfjaseifj.mp4\n",
            "2_sdafkjaslkclaksdjkas.mp4\n",
            "2_sdfkjsaleijflaskdjf.mp4\n",
            "2_sdjfhafsldkjhjk.mp4\n",
            "2_sdkjdsflkjfwa.mp4\n",
            "2_sdlfjlewlkjkj.mp4\n",
            "2_sdlkjsaelijfksdjf.mp4\n",
            "3_asldkfjalwieaskdfaskdf.mp4\n",
            "3_dkk873lkjlksajdf.mp4\n",
            "3_dsjlaeijlksjdfie.mp4\n",
            "3_dsksdfjbvsdkj.mp4\n",
            "3_dslkaldskjflakjs.mp4\n",
            "3_ewdfkjwaeoihjlkasdjf.mp4\n",
            "3_kling_20251205_Text_to_Video_In_a_grass_4697_0.mp4\n",
            "3_kling_20251205_Text_to_Video_On_a_playg_5028_0.mp4\n",
            "3_kling_20251205_Text_to_Video_On_a_playg_5064_0.mp4\n",
            "3_kling_20251206_Text_to_Video_Generate_a_17_0.mp4\n",
            "3_kling_20251206_Text_to_Video_Generate_a_315_0.mp4\n",
            "3_kling_20251206_Text_to_Video_Generate_a_315_2.mp4\n",
            "3_kling_20251206_Text_to_Video_Generate_a_712_3.mp4\n",
            "3_kling_20251206_Text_to_Video_Generate_a_71_0.mp4\n",
            "3_kling_20251206_Text_to_Video_Generate_a_71_2.mp4\n",
            "3_kling_20251206_Text_to_Video_Generate_a_71_3.mp4\n",
            "3_kling_20251209_Image_to_Video_Generate_a_613_1.mp4\n",
            "3_kling_20251209_Image_to_Video_Generate_a_635_0.mp4\n",
            "3_kling_20251209_Text_to_Video_Generate_a_190_1.mp4\n",
            "3_kling_20251209_Text_to_Video_Generate_a_403_1.mp4\n",
            "3_kling_20251209_Text_to_Video_Generate_a_491_0.mp4\n",
            "3_kling_20251209_Text_to_Video_Generate_a_491_1.mp4\n",
            "3_kling_20251209_Text_to_Video_Generate_a_491_2.mp4\n",
            "3_kling_dskfseu.mp4\n",
            "3_kling_kdjflaskdjf.mp4\n",
            "3_sadklfjasbnlkjlfkj.mp4\n",
            "3_sadlfkjasldkfjasleijlkjfd.mp4\n",
            "3_sadlfkjawelnflksdjf.mp4\n",
            "3_sdfjwaiejflkasjdf.mp4\n",
            "3_sdflkjliejkjdf.mp4\n",
            "3_sdlkfjaleknaksej.mp4\n",
            "3_sdlkfjalkjejafe.mp4\n",
            "3_sdlkjfaslkjfalskjdf.mp4\n",
            "3_sdlkjslndflkseijlkjef.mp4\n",
            "4_20251209_Text_to_Video_Generate_a_561_0.mp4\n",
            "4_asdlkfjalsflnekj.mp4\n",
            "4_aslkcasckmwlejk.mp4\n",
            "4_aslkjasmcalkewjlkje.mp4\n",
            "4_dssalsdkfjweijf.mp4\n",
            "4_kling_20251206_Text_to_Video_Generate_a_28_0.mp4\n",
            "4_kling_20251206_Text_to_Video_Generate_a_315_3.mp4\n",
            "4_kling_20251206_Text_to_Video_Generate_a_58_0.mp4\n",
            "4_kling_20251207_Text_to_Video_Generate_a_521_1.mp4\n",
            "4_kling_20251209_Image_to_Video_Generate_a_635_1.mp4\n",
            "4_kling_20251209_Text_to_Video_Generate_a_190_0.mp4\n",
            "4_kling_20251209_Text_to_Video_Generate_a_218_0.mp4\n",
            "4_kling_20251209_Text_to_Video_Generate_a_263_1.mp4\n",
            "4_kling_20251209_Text_to_Video_Generate_a_377_1.mp4\n",
            "4_kling_20251209_Text_to_Video_Generate_a_452_0.mp4\n",
            "4_kling_20251209_Text_to_Video_Generate_a_452_1.mp4\n",
            "4_kling_20251209_Text_to_Video_Generate_a_561_1.mp4\n",
            "4_kling_20251209_Text_to_Video_Generate_a_588_2.mp4\n",
            "4_pushup_1f2da596-7619-4d55-9376-069e15a42a1a_h264.mp4\n",
            "4_sadflkjasldkjfalseij.mp4\n",
            "4_sadlfkjlknewkjejk.mp4\n",
            "5_sadfjhaslfkjasdlkfjsa.mp4\n",
            "5_sdfkljweoijlkjdsflkjweaij.mp4\n",
            "6_dfjewaijsldkjfsaef.mp4\n",
            "6_kling_20251209_Text_to_Video_Generate_a_218_1.mp4\n",
            "7_sadkjfkljekj.mp4\n",
            "\n",
            "Total: 77 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These videos are now available in the folder \"video-data\". You can click on the folder icon on the left-hand-side of this screen to see the videos in a file explorer."
      ],
      "metadata": {
        "id": "5wPAlvHdXj3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create your Datasets and Dataloaders"
      ],
      "metadata": {
        "id": "vtisgbeiYiH_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some example code for approaching the first *two* TODOs is given below just to get you started. No starter code is given for the third TODO.\n",
        "\n",
        "Note, the below code is very rough skeleton code. Make no assumptions as to the correct manner to architect your model based on the structure of this code.\n",
        "\n",
        "Please feel free to (if not encouraged to) change every single line of the below code (change it to best suit your chosen model architecture, in the next section)."
      ],
      "metadata": {
        "id": "Xo9J9hXLeCdY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO 1 (This is mostly already done for you - Please see the v1 provided below)\n",
        "\n",
        "Each video in the folder is prefixed by a number. That number corresponds to the number of distinct pushups visible in the video. Write code to iterate over each video in the folder, and extract the corresponding target associated with the video."
      ],
      "metadata": {
        "id": "XzDMxGLnYa0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO 2 (This is also mostly already done for you - Please see the v1 provided below)\n",
        "\n",
        "\n",
        "Divide the data into training and validation sets.\n",
        "\n",
        "Optionally, you can also create out your own test set to assess your performance."
      ],
      "metadata": {
        "id": "74PvwbsYYMlD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO 3\n",
        "\n",
        "Any preprocessing or augmentation of your data which you deem required, should (probably) go here. You are also free to include your data-augmentation code later, though doing it before creating your dataloaders is probably a good idea.\n",
        "\n",
        "If you complete this TODO, to maintain experimental hygiene, feel free to modify the code which was provided for TODOs 1 and 2."
      ],
      "metadata": {
        "id": "hEaV_5oXZQRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here is a basic implementation of the above two TODOs. You can assume the first TODO is completed correctly.\n",
        "\n",
        "# Please modify this code to suit you best, as you decide on your preferred model architecture.\n",
        "\n",
        "# For example, below here we are padding every video to 1,000 frames. That may or may not be a good idea.\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class VideoDataset(Dataset):\n",
        "    \"\"\"Dataset for loading videos from a folder. Labels from filename prefix.\"\"\"\n",
        "\n",
        "    def __init__(self, video_dir, frame_size=(224, 224), transform=None):\n",
        "        self.video_dir = video_dir\n",
        "        self.frame_size = frame_size\n",
        "        self.transform = transform\n",
        "\n",
        "        self.video_files = [\n",
        "            f for f in os.listdir(video_dir)\n",
        "            if f.endswith(('.mp4', '.avi', '.mov'))\n",
        "        ]\n",
        "\n",
        "        self.labels = [\n",
        "            int(f.split('_')[0]) for f in self.video_files\n",
        "        ]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path = os.path.join(self.video_dir, self.video_files[idx])\n",
        "        frames = self._load_video(video_path)\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            frames = self.transform(frames)\n",
        "\n",
        "        return frames, label\n",
        "\n",
        "    def _load_video(self, path):\n",
        "        cap = cv2.VideoCapture(path)\n",
        "\n",
        "        frames = []\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frame = cv2.resize(frame, (self.frame_size[1], self.frame_size[0]))\n",
        "            frames.append(frame)\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "        frames = torch.from_numpy(np.array(frames)).permute(3, 0, 1, 2).float() / 255.0\n",
        "\n",
        "        return frames\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Pad all videos to 1000 frames.\"\"\"\n",
        "    frames_list, labels = zip(*batch)\n",
        "\n",
        "    target_frames = 1000\n",
        "\n",
        "    padded_frames = []\n",
        "    for frames in frames_list:\n",
        "        num_frames = frames.shape[1]\n",
        "        if num_frames < target_frames:\n",
        "            padding = torch.zeros(frames.shape[0], target_frames - num_frames, frames.shape[2], frames.shape[3])\n",
        "            frames = torch.cat([frames, padding], dim=1)\n",
        "        elif num_frames > target_frames:\n",
        "            frames = frames[:, :target_frames, :, :]\n",
        "        padded_frames.append(frames)\n",
        "\n",
        "    frames_batch = torch.stack(padded_frames, dim=0)\n",
        "    labels_batch = torch.tensor(labels)\n",
        "\n",
        "    return frames_batch, labels_batch\n",
        "\n",
        "\n",
        "def get_dataloaders(video_dir, batch_size=4, val_split=0.2, frame_size=(224, 224)):\n",
        "    \"\"\"Create train and validation dataloaders.\"\"\"\n",
        "\n",
        "    full_dataset = VideoDataset(video_dir, frame_size=frame_size)\n",
        "\n",
        "    val_size = int(len(full_dataset) * val_split)\n",
        "    train_size = len(full_dataset) - val_size\n",
        "\n",
        "    train_dataset, val_dataset = random_split(\n",
        "        full_dataset,\n",
        "        [train_size, val_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=0,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    print(f\"Train: {len(train_dataset)} videos, Val: {len(val_dataset)} videos\\n\")\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "video_dir = './video-data'\n",
        "\n",
        "train_loader, val_loader = get_dataloaders(video_dir, batch_size=4, val_split=0.2)\n",
        "\n",
        "for frames, labels in train_loader:\n",
        "    print(f\"Frames shape: {frames.shape}\")  # (B, C, 1000, H, W)\n",
        "    print(f\"Labels: {labels}\")\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvqxH1YBYCUw",
        "outputId": "e27c4b8f-b316-4313-d7bd-3cf63602e428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 62 videos, Val: 15 videos\n",
            "\n",
            "Frames shape: torch.Size([4, 3, 1000, 224, 224])\n",
            "Labels: tensor([4, 2, 3, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a Model"
      ],
      "metadata": {
        "id": "YVPYRadrZdty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this assignment, we request you use PyTorch. Below is an example of how to instantiate a very basic PyTorch model.\n",
        "\n",
        "Note, this model below needs a _lot_ of work.\n",
        "\n",
        "Please include your code for creating your model below.\n",
        "\n",
        "The only constraint here is that you define a Python object which inherits from a PyTorch nn.Module object. Beyond that, please feel free to implement anything you like: Transformer, Vision Transformer, MLP, CNN, etc."
      ],
      "metadata": {
        "id": "UrpSDGMWaBR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO 4\n",
        "\n",
        "Create your model."
      ],
      "metadata": {
        "id": "sRolEQeAbxsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from huggingface_hub import HfApi, hf_hub_download\n",
        "\n",
        "\n",
        "class SimpleVideoClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        # Average over frames, then use a simple CNN\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.AdaptiveAvgPool2d((4, 4))\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(64 * 4 * 4, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, T, H, W)\n",
        "        # Average over time dimension\n",
        "        x = x.mean(dim=2)  # (B, C, H, W)\n",
        "        x = self.conv(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "H5FlYz3paNxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train your Model"
      ],
      "metadata": {
        "id": "3Bou97f8czAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO 5\n",
        "\n",
        "Training time! Please include your training code below.\n",
        "\n",
        "As per above, please feel free (and encouraged) to rip out all of the below code and replace with your (much better) code.\n",
        "\n",
        "The below should just be used as an example to get you started."
      ],
      "metadata": {
        "id": "VtSqu_ZkcFxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        pred = output.argmax(dim=1)\n",
        "        correct += pred.eq(target).sum().item()\n",
        "        total += target.size(0)\n",
        "\n",
        "    return total_loss / len(train_loader), correct / total\n",
        "\n",
        "\n",
        "def evaluate(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            total_loss += criterion(output, target).item()\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += pred.eq(target).sum().item()\n",
        "            total += target.size(0)\n",
        "\n",
        "    return total_loss / len(test_loader), correct / total\n",
        "\n",
        "\n",
        "def train_model(epochs=5, lr=1e-3):\n",
        "    \"\"\"Train and return your model.\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\\n\")\n",
        "\n",
        "    model = SimpleVideoClassifier().to(device)\n",
        "    print(\"Instantiated model.\\n\")\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    train_loader, val_loader = get_dataloaders(video_dir='./video-data')\n",
        "    print(\"Got dataloaders.\\n\")\n",
        "\n",
        "    print(\"Go time. Let the training commence.\\n\")\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f\"Epoch {epoch}/{epochs} | \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "EueH4HSdcLlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting a manual seed for reproducibility\n",
        "torch.manual_seed(0)\n",
        "model = train_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHI68u4XhCGB",
        "outputId": "c28ba0d4-cb87-4076-eb13-ab93d597ef10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Instantiated model.\n",
            "\n",
            "Train: 62 videos, Val: 15 videos\n",
            "\n",
            "Got dataloaders.\n",
            "\n",
            "Go time. Let the training commence.\n",
            "\n",
            "Epoch 1/5 | Train Loss: 1.8303, Train Acc: 0.3871 | Val Loss: 1.2600, Val Acc: 0.0667\n",
            "Epoch 2/5 | Train Loss: 1.5231, Train Acc: 0.3226 | Val Loss: 1.1248, Val Acc: 0.6667\n",
            "Epoch 3/5 | Train Loss: 1.4453, Train Acc: 0.3871 | Val Loss: 1.1796, Val Acc: 0.6667\n",
            "Epoch 4/5 | Train Loss: 1.4244, Train Acc: 0.3871 | Val Loss: 1.1842, Val Acc: 0.6667\n",
            "Epoch 5/5 | Train Loss: 1.4410, Train Acc: 0.3871 | Val Loss: 1.1902, Val Acc: 0.6667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "W7gmJS-yn2qc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO 6\n",
        "\n",
        "Include any code which you feel is useful for evaluating your model performance below."
      ],
      "metadata": {
        "id": "hzS5ADbDn6Yr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "y1KwRou4oCkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging Face"
      ],
      "metadata": {
        "id": "eAmXb-QC2ChR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is a requirement of this assignment that you submit your trained model to a repo on Hugging Face, and make it publicly available. Below, we provide code which should help you do this."
      ],
      "metadata": {
        "id": "cl3rU9Ec4uSI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO 7\n",
        "\n",
        "Upload your model to HuggingFace"
      ],
      "metadata": {
        "id": "hSUcEj-DoI8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the dependencies:"
      ],
      "metadata": {
        "id": "qtUkkHpCtQaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYdo05DftcBC",
        "outputId": "42444b1b-cb84-4893-f6e2-c17cc829f37c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll now need to log in to Hugging Face via the command line. To do this, you'll need to generate a token on your Hugging Face account. To generate a token, run the below command, and click on the link which appears."
      ],
      "metadata": {
        "id": "XIlD5u1U5IBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hf auth login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9d3loOxtf7v",
        "outputId": "c26a78c2-a0f1-4298-ad28-3ad896050af4"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code will only run if you have already trained a model with variable name 'model'.\n",
        "\n",
        "The below code will take your trained model, and upload it to a *public* HuggingFace repo in your account called \"mv-final-assignment\".\n",
        "\n",
        "(Note - in this example, we have set 'private=False' in the upload_to_hub method. This makes your model public).\n",
        "\n",
        "You should double-check that your model is in fact public. To do that, you can navigate (in an incognito tab, in a browser) to https://huggingface.co/YOUR_USERNAME/YOUR_MODEL_NAME and see if that page loads. If your model is public, it will. (Simply being able to run the below code will not guarantee that your model is in fact public, because, you have now authenticated yourself with the huggingface CLI)."
      ],
      "metadata": {
        "id": "IEnw3O5I5kY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR HUGGING FACE USERNAME BELOW\n",
        "hf_username = 'rossamurphy'"
      ],
      "metadata": {
        "id": "lCq_stTaoeQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from huggingface_hub import HfApi, hf_hub_download\n",
        "\n",
        "\n",
        "def save_model(model, path=\"model.pt\"):\n",
        "    \"\"\"Save the model weights to a file.\"\"\"\n",
        "    torch.save(model.state_dict(), path)\n",
        "    print(f\"Model saved to {path}\")\n",
        "\n",
        "\n",
        "def upload_to_hub(local_path=\"model.pt\", repo_id=f\"{hf_username}/mv-final-assignment\"):\n",
        "    \"\"\"\n",
        "    Upload model to Hugging Face Hub.\n",
        "\n",
        "    Args:\n",
        "        local_path: Path to your saved model file\n",
        "        repo_id: Your repo in format \"username/model-name\"\n",
        "    \"\"\"\n",
        "    api = HfApi()\n",
        "\n",
        "    # Create the repo first (if it already exists, this will just skip)\n",
        "    api.create_repo(\n",
        "        repo_id=repo_id,\n",
        "        repo_type=\"model\",\n",
        "        exist_ok=True,  # Don't error if it already exists\n",
        "        private=False,  # Make it public so TAs can access\n",
        "    )\n",
        "\n",
        "    # Now upload the file\n",
        "    api.upload_file(\n",
        "        path_or_fileobj=local_path,\n",
        "        path_in_repo=\"model.pt\",\n",
        "        repo_id=repo_id,\n",
        "        repo_type=\"model\",\n",
        "    )\n",
        "\n",
        "    print(f\"Model uploaded to https://huggingface.co/{repo_id}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# EXAMPLE USAGE\n",
        "# =============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    save_model(model, \"mv-final-assignment.pt\")\n",
        "\n",
        "    upload_to_hub(\"mv-final-assignment.pt\", f\"{hf_username}/mv-final-assignment\")\n"
      ],
      "metadata": {
        "id": "_AdQof5XtWfS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}